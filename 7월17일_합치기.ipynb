{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score   # 소문자로 시작하면 함수 : cross_val_score ,  대문자 시작 클래스 :LinearRegression\n",
    "from sklearn.datasets import load_iris # 분류\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression \n",
    "# LinearRegression => 예측용, LogisticRegression => 분류용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = pd.get_dummies(loans) # 카테고리를 원핫 인코딩 => 성별값을 컬럼으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/ Test용 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p47 k-최근접 이웃 모델 : \n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNeighborsClassifier => 대문자로 시작 : 클래스."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1) # 객체 생성, Classifier:분류기\n",
    "# p48 훈련 데이터셋으로 모델을 만드는 .fit 함수 : 모델을 훈련 데이터에 맞게 핏팅시킴\n",
    "knn.fit(x_train, y_train)  # x_train의 훈련한 값의 정답은 y_train이라고 핏팅시켰음.\n",
    "# p49 클래스를 예측(분류) 함.\n",
    "prediction = knn.predict(x_new) \n",
    "knn.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선택, 학습  : Ridge은 예측 => 삼성전자, 비트코인\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "rd= Ridge().fit(X_train, y_train)\n",
    "# 성능 측정\n",
    "lr.score(X_train, y_train), lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미래의 5일 환율 예측을 그래프로 시각화\n",
    "최근20일데이터 = 현재가.iloc[-window_day:].values.reshape([1,-1])\n",
    "미래의5일예측 = lr.predict(최근20일데이터)\n",
    "미래의5일예측 = np.reshape(미래의5일예측,predict_day)\n",
    "미래의5일예측.shape\n",
    "# 예측을 그리프로 시각화\n",
    "x_num = np.arange(len(현재가))\n",
    "x1_num = np.arange(len(현재가), len(현재가) + len(미래의5일예측))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/program Files (x86)/Graphviz2.38/bin/'\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=5).fit(X,y)\n",
    "export_graphviz(model, out_file='loan.dot', class_names=['ok', 'not paid'],\n",
    "               feature_names=X.columns, impurity=False, filled=True) \n",
    "with open('loan.dot') as f:\n",
    "    dot_graph = f.read()\n",
    "g = graphviz.Source(dot_graph)    \n",
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier   # 결정트리\n",
    "from sklearn.ensemble import RandomForestClassifier  # 랜덤 포레스트\n",
    "rf = RandomForestClassifier(n_estimators=1000).fit(X_train, y_train)  # 부트스트랩 : 데이터가 중복으로 뽑힘\n",
    "# 성능 좋고, overfitting이 안됨. 단점은 tree모델이라 컬럼이 많으면 잘 안됨. 0이 많으면 잘안됨.\n",
    "# n_estimators 을 최대한 많이 할 수록 좋음\n",
    "rf.score(X_test, y_test)   # n_estimators=10 => 0.965034965034965   , n_estimators=100 => 실행할 때마다 변경됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그래디언트 부스팅(Gradient Boosting)\n",
    "- 트리 앙상블\n",
    "- 랜덤포레스트 : 여러 나무가 서로 다른 데이트로 독립적으로 학습(병렬 가능)\n",
    "- 그래디언트 부스팅 회귀 트리 : 나무 하나씩 순차적으로 학습, 이전 나무의 오류를 최소화하는 방향으로 학습\n",
    "- 이전 오차를 최소화 하는 방향으로 다음 나무 생성하여 학습함\n",
    "  - 작은 나무 : 1~5개\n",
    "     - 나무가 작다(max_depth가 작고, leaf 가 적다) => 성능이 떨어짐. => 약한 학습기(weak learner)\n",
    "- 보통 안정적인 랜덤 포레스트 먼저 사용하고 더 성능을 놓이고자 할 때 그래디언트 부스팅, xgboost을 사용\n",
    "- 단점 : 매개변수 조정이 어렵다. 훈련 시간이 길다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p 122\n",
    "from sklearn.ensemble import GradientBoostingClassifier # 그래디언트 부스팅\n",
    "gbc = GradientBoostingClassifier(random_state=0).fit(X_train, y_train)\n",
    "gbc.score(X_train,y_train), gbc.score(X_test,y_test) # 과적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "과적합 -> 트리를 작게 만들자 -> 가지치기(사전/사후 가지치기)\n",
    "사전가지치기는 미리 자르기, 사후가지치기는 나무를 다 만들고 leaf를 없앰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p131 에이다부스트(AdaBoost) : Adaptive Boosting\n",
    "- 그라디언트 부스트 트리와 비슷\n",
    "- 차이 \n",
    "\n",
    "      - GB : 이전 트리의 오차를 최소화\n",
    "      - AB : 이전 트리의 오차 샘플에 가중치를 높여서 다음 트리에서 학습\n",
    "- 결정 트리 -> 과적합 -> 앙상블(랜덤 포레스트, 그라디언트 부스팅 or 에이다부스트)\n",
    "- 최종결정 : 분류 작업일 경우는 투표(다수결), 예측일 때는 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adc = AdaBoostClassifier(random_state=0).fit(X_train,y_train)\n",
    "adc.score(X_train,y_train), adc.score(X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM(Support vector machine)\n",
    "- 139 학습 : 일부 데이터 포인트(샘플)가 클래스 사이의 결정 경계를 구분하는데 얼마나 중요한지를 배운다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p139\n",
    "from sklearn.svm import SVC, SVR # SCM-C분류용, SVM-R회귀용\n",
    "import mglearn\n",
    "# 140\n",
    "g = 1  # gamma : 하나의 샘플이 미치는 영향의 범위, 작으면 넓은 범위(많은 샘플:일반화)\n",
    "c = 1  # 규제 regularization : 파라미터 값이 작아지도록 규제(다양한 특징 :일반화)\n",
    "X, y = mglearn.tools.make_handcrafted_dataset()\n",
    "svm = SVC(kernel='rbf', C=c, gamma=g).fit(X,y)\n",
    "mglearn.plots.plot_2d_separator(svm, X, eps=.5)\n",
    "sv = svm.support_vectors_\n",
    "sv_labels = svm.dual_coef_.ravel() > 0\n",
    "mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n",
    "sv = SVC(gamma='auto').fit(X_train,y_train)\n",
    "sv.score(X_train,y_train), sv.score(X_test,y_test) \n",
    "# 값을 정규화하면 95%까지 상승, C를 올리면 97%까지 상승"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p151\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "mlp = MLPClassifier().fit(X_train,y_train)\n",
    "mlp.score(X_train,y_train), mlp.score(X_test,y_test)  # 과소적합 : under fitting : 성능이 안나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3,3) 히든레이어 2개에 각각 노드 3개씩\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,50,20)).fit(X_train,y_train)\n",
    "mlp.score(X_train,y_train), mlp.score(X_test,y_test) \n",
    "# 장점 : 모델 형태를 내 마음대로 설계, 성능은 좋은(good)\n",
    "# 단점 : 어떤 형태가 최선인지 모름. 학습 속도가 느리고, 분석이 까다롭다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (87 * 65)개 이상의 명암을 주성분 100(10*10)개로 변환해서 시각화\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10*10, whiten=True).fit(people.data)   # whiten 옵션은 정규화를 시킴.\n",
    "pca_x = pca.transform(people.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류기를 만들어서 분류하기\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = people.data\n",
    "y = people.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y)\n",
    "model = RidgeClassifier().fit(X_train, y_train)\n",
    "model.score(X_train, y_train), model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2).fit(digits.data)   \n",
    " #64차원을 2차원으로 축소 => 64개 중 가장 특징 있는 것 2개로 나타냄.\n",
    "pca_x = pca.transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p223\n",
    "from sklearn.manifold import TSNE\n",
    "digits_tsne = TSNE().fit_transform(digits.data)\n",
    "l = mglearn.discrete_scatter(digits_tsne[:,0], digits_tsne[:,1], digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터(군집화), DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "digit_tsme = TSNE().fit_transform(digits.data)\n",
    "km = KMeans(n_clusters=10).fit(digit_tsme)  # x를 3개로 그룹화(cluster)\n",
    "predict_group = km.predict(digit_tsme)      # x가 몇번째 그룹이니?\n",
    "l = mglearn.discrete_scatter(digit_tsme[:,0], digit_tsme[:,1], predict_group)\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "model = AgglomerativeClustering(n_clusters=3)\n",
    "p = model.fit_predict(x)  #  predict\n",
    "mglearn.discrete_scatter(x[:,0], x[:,1],p)\n",
    "mglearn.plots.plot_dbscan()\n",
    "\n",
    "p = DBSCAN().fit_predict(x)   # k 개를 정해주지 않음.\n",
    "l = mglearn.discrete_scatter(x[:,0], x[:,1], p)\n",
    "p = DBSCAN(eps=1).fit_predict(x)   # k 개를 정해주지 않음. \n",
    "l = mglearn.discrete_scatter(x[:,0], x[:,1], p)\n",
    "\n",
    "# min_samples ; 클러스터가 되기 위한 최소 샘플수\n",
    "# eps : 클러스터의 샘플들간의 거리\n",
    "p = DBSCAN(eps=2, min_samples=5).fit_predict(x)   # k 개를 정해주지 않음. \n",
    "l = mglearn.discrete_scatter(x[:,0], x[:,1], p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
